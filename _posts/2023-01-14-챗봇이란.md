---
layout: single
title: "챗봇에 대해서..."
toc: true
toc_sticky: true
toc_label: "목차"
categories: AI
excerpt: ""
tag: [ai]
---

# 📘 챗봇에 대해서...
챗봇은 우리가 흔히 알고 있듯이 인공지능 기술을 통해 대화를 주고 받읋 수 있는 프로그램이다.  
챗봇에 대해 공부하면서 알게된 것들과 사용되는 기술들에 대해서 적어볼까 한다!  
*"한빛네트워크-처음 배우는 딥러닝 챗봇"을 통해 공부한 내용을 바탕으로 챕터 별로 정리했음*  

이에 관련된 코드들과 챕터 별 내용(.md파일)들은 아래 링크에 있다.  
[https://github.com/hellojunho/chatbot](https://github.com/hellojunho/chatbot)  

이 책에서는 1장에서는 챗봇의 소개 및 사례, 2장에서는 파이썬 기본 문법에 대해서 다루고, 3장부터 본격적인 챗봇 개발 관련 내용이다.  
이 포스팅은 이 책의 전체 내용을 다룬다.  
내가 이 책을 읽으며 실습했던 코드와 그 개념들을 챕터 별로 기록하며 공부했던 내용들을 한 번에 담은 것이라 매우 길다.  
책 한권의 분량이 들어가 있다.  

# 부록 A. 개발 환경 구축
개인적으로 시간이 가장 많이 걸렸던 부분이다.  
아나콘다, 파이썬, 가상환경을 사용해서 구축한다.  

## A.1 파이썬 설치하기
[macOS]  
> brew install python@3.7
    
       
[windows]  
[www.python.org](https://www.python.org/)에 접속해서 파이썬 다운로드 후 설치  

## A.2 아나콘다 설치하기
[https://www.anaconda.com/products/distribution](https://www.anaconda.com/products/distribution) 에 접속해서 운영체제에 맞는 버전 설치  

- 아나콘다 실행 확인
[macOS]  
> conda list

[windows]  
> conda list

## A.3 CLI 환경에서 콘다로 가상 환경 만들기
가상환경은 바구니라고 생각할 수 있을 것 같다.  
한 바구니 안에 패키지를 담으면(설치하면) 다른 바구니와는 섞이지 않는다.  
그래서 다른 환경과 패키지 충돌을 일으키지 않게 사용할 수 있다는 장점이 있다.  

[가상환경 생성]  
> conda create -n [가상환경 이름] python=[파이썬 버전]

가상환경 이름을 `chatbot`, 파이썬 버전은 3.7로 하면,  
> conda create -n chatbot python=3.7

[가상환경 활성화]  
> conda activate [가상환경 이름]

활성화되면, 콘솔 창에 아래와 같이 나올 것이다.  

```
(base) # conda activate chatbot
(chatbot) #
```

[가상환경 비활성화]
> conda deactivate  

## A.4 기타 패키지 설치하기
`pip`명령어로 아래 패키지들을 설치해보자!  
1. tensorflow
> pip install tensorflow==2.1
2. konlpy
> pip install konlpy
3. PyKomoran
> pip install PyKomoran
4. gensim
> pip install gensim
5. sklearn
> pip install sklearn
6. seqeval
> pip install seqeval
7. PyMySQL
> pip install PyMySQL
8. openxl
> pip install openxl
9. pandas, xlrd
> pip install pandas, xlrd
10. matplotlib
> pip install matplotlib
11. flask
> pip install flask
12. requests
> pip install requests

# 3장. 토크나이징
컴퓨터가 자연어 의미를 분석해 컴퓨터가 처리할 수 있도록 하는 일을 `자연어 처리`라고 한다.  

## 자연어 처리란
`자연어 처리`란, `Natural Language Processing`의 약자로 `NLP`라고 부른다.  
챗봇도 자연어 처리의 한 분야임!  

어떤 방식으로 자연어를 컴퓨터에게 학습시킬까?  
먼저 문장을 일정한 의미가 있는 `가장 작은 단어`들로 나누어, 그 단어들을 이용해 분석한다.  
여기서 `가장 작은 단어`가 바로 `토큰(token)`이다!!  

## 토크나이징
위에서 `토큰`이 무엇인지 알아봤는데, `토크나이징`은 말 그대로 `토큰화`하는 과정이라고 이해했다!  
여기서는 `KoNLPy(코엔엘파이)` 라이브러리를 사용했음!!  

## 3.2. KoNLPy
`KoNLPy`는 기본적인 한국어 자연어 처리를 위한 파이썬 라이브러리이다.  
여기서는 한국어 문장을 토크나이징 작업을 제일 먼저 수행할건데, 토큰 단위를 `형태소`를 기본으로 하여 토큰화 할 것이다.  

## 3.2.1 Kkma
`Kkma`는 `꼬꼬마`라고 부름!  
꼬꼬마 형태소 분석기를 사용하려면 `konlpy.tag`패키지의 `Kkma`모듈을 불러와야 함.  

### Kkma 모듈 함수

morphs(pharse) : 인자로 입력한 문장을 `형태소` 단위로 토크나이징함. 토크나이징된 형태소들은 `리스트` 형태로 반환.  
nouns(pharse)  : 인자로 입력한 문장에서 `명사`인 토큰만 추출.  
pos(pharse, flatten=True) : `POS tagge`라고 하며, 인자로 입력한 문장에서 형태소를 추출한 뒤, 품사 `태깅`을 함. `튜플`형태로 묶여서 `리스트`형태로 반환.  
sentences(pharse) : 인자로 입력한 여러 문장을 분리해줌. `리스트` 형태로 반환.  


[Kkma 품사 태그]  
NNG : 일반 명사  
JKS : 주격 조사  
JKM : 부사격 조사  
VV  : 동사  
EFN : 평서형 종결 어미  
SF  : 마침표, 물음표, 느낌표  


## 3.2.2 Komoran
`Komoran`은 자바로 개발한 한국어 형태소 분석기이다.  
`코모란`이라고 부르고, 다른 분석기와는 다르게 `공백이 포함된 형태소`단위로도 분석 가능해 자주 쓰임!!  
이걸 쓰려면 코모란 모듈을 불러와야 함!  

```
from konlpy.tag import Komoran
```

### Komoran 모듈의 함수 설명
morphs(pharse) : 인자로 입력한 문장을 형태소 단위로 토크나이징함. `리스트`형태로 반환.  
nouns(pharse)  : 인자로 입력한 문장에서 `명사`들만 추출.  
pos(pharse, flatten=True) : `POS tagge`라고 하며, 인자로 입력한 문장에서 형태소를 추출한 뒤, 품사 `태깅`을 함. `튜플`형태로 묶여서 `리스트`형태로 반환.  


## 3.2.3 Okt
`Okt`는 트위터에서 개발한 한국어 처리기이다.  
얘도 쓰려면 `Okt`모듈을 불러와야 함.  
```
from konlpy.tag import Okt
```

### Okt모듈의 함수
morphs(pharse)  
nouns(pharse)  
pos(pharse, stem=Falsem join=False)  
nomalize(pharse) : 입력한 문장을 정규화함.  ex) 사랑햌ㅋ -> 사랑해 ㅋㅋ  
pharses(pharse) : 입력한 문장에서 어구를 추출함.  ex) 오늘 날씨가 좋아요. -> ['오늘', '오늘 날씨', '날씨']  
 
### Okt 품사 태그 표
Noun : 명사  
Verb : 동사  
Adjective : 형용사  
Josa : 조사  
Punctuation : 구두점  


# 4장. 임베딩
컴퓨터는 자연어를 직접적으로 처리할 수 없음!  
수치연산만 가능해서 자연어를 숫자나 벡터 형태로 변환해야 하는데, 이런 과정을 `임베딩`이라고 함.  
즉, 단어나 문장을 수치화해 벡터 공간으로 표현하는 과정을 말함.  
임베딩 기법에는 `문장 임베딩`과 `단어 임베딩`이 있음!  


## 4.2. 단어 임베딩
`단어 임베딩`은 말뭉치에서 각각의 단어를 벡터로 변환하는 기법을 말함.  
단어 임베딩은 의미와 문법적 정보를 지니고 있음.  

### 4.2.1 원-핫 인코딩
`원-핫-인코딩`은 단어를 숫자 벡터로 변환하는 가장 기본적인 방법임.  
요소들 중 단 하나의 값만 1이고 나머지 요솟값은 0인 인코딩을 의미함.  
원-핫-인코딩으로 나온 결과를 `원-핫-벡터`라고 하고, 전체 요소 중 단 하나만 1이기 때문에 `희소 벡터`라고 함!  

### 4.2.2 희소 표현과 분산 표현
단어가 희소벡터로 표현되는 방식을 `희소 표현`이라고 한다.  
희소 표현은 각각의 차원이 독립적인 정보를 지니고 있어 사람이 이해하기에 직관적인 장점, 단어 간의 연관성이 전혀 없어 의미를 담을 수 없고, 단어 사전의 크기가 커질 수록 메모리 낭비와 계산 복잡도가 커지는 단점.  

자연어 처리를 잘하기 위해서는 기본 토큰이 되는 단어의 의미와 주변 단어 간의 관계가 단어 임베딩에 표현되어야 함.  
이를 해결하기 위해 단어 간의 유사성을 잘 표현하면서도 벡터 공간을 절약할 수 있는 방법을 고안했는데, 이를 `분산 표현`이라고 함! 

### 4.2.3 Word2Vec
원-핫-인코딩의 경우에는 구현이 간단하지만, 챗봇의 경우에는 단어간의 유사도를 계산할 수 있어야 좋은 성능을 낸다는 단점이 있다.  
그래서 챗봇의 경우에는 원-핫-인코딩은 좋은 기법은 아니다.  
그렇기 때문에! 분산 표현 형태의 단어 임베딩 모델을 사용할 것이다.  
대표적인 모델로는 `Word2Vec`모델이 있다.  


# 5장. 텍스트 유사도
Word2Vec은 인공 신경망을 이용했다.  
이번엔 통계적인 방법을 이용해 유사도를 계산해보자.  

## 5.1. n-gram 유사도
`n-gram`은 주어진 문장에서 n개의 연속적인 단어 시퀀스를 의미한다.  
n-gram은 문장에서 n개의 단어를 토큰으로 사용한다.  
이 것은 `이웃한 단어`의 `출현 횟수`를 통계적으로 표현해 텍스트의 `유사도`를 계산하는 방법이다.  

[n에 따른 n-gram]  
```
'1661년' '6월' '뉴턴'은 '선생님'의 '제안'으로 '트리니티'에 '입학'하였다.  

n=1 (unigram) : 1661년/6월/뉴턴/선생님/제안/트리니티/입학  
n=2 (bigram) : 1661년 6월/6월 뉴턴/뉴턴 선생님/선생님 제안/... /트리니티 입학  
...
n=4 (4-gram) : 1661년 6월 뉴턴 선생님/6월 뉴턴 선생님 제안/뉴턴 선생님 제안 트리니티/../선생님 제안 트리니티 입학  
n=7 : 1661년 6월 뉴턴 선생님 제안 트리니티 입학
```
n=1 : unigram, n=2 : bigram, n=3 : trigram, n=4 : 4-gram ...

문장 A와 B가 있다고 하자.  
문장 B가 A와 얼마나 유사할까?  
similarity = tf(A, B)/tokens(A)  
- n-gram 유사도 공식
    - tf(A, b) : A와 B에서 동일한 토큰의 출현 빈도
    - tokens()는 해당 문장이 전체 토큰 수

[2-gram을 이용한 예시]  
A: 6월에 뉴턴은 선생님의 제안으로 트리니티에 입학했다.  
B: 6월에 뉴턴은 선생님의 제안으로 대학교에 입학했다.  

- 문장 A에서는 6개의 토큰이 전부 나온다.  
- 문장 B에서는 동일한 토큰이 4개 카운트 되었다.  
- 즉, `n-gram 유사도 수식`에 따라 4/6으로 `0.66`의 유사도를 지닌다. == "B는 A와 66% 유사합니다."  


## 5.2. 코사인 유사도
앞서, 단어나 문장을 벡터로 표현할 수 있었다.  
그렇다면 벡터 간의 거리나 각도를 이용해 유사성을 파악할 수도 있다.  
벡터 간 거리를 구하는 방법은 다양하지만, 여기서는 `코사인 유사도`를 사용할 것이다.  
`코사인 유사도`는 두 벡터 간 코사인 각도를 이용해 유사도를 측정하는 방법이다.  
이 유사도는 벡터의 크기가 중요하지 않을 때 그 거리를 측정하기 위해 사용된다.  

similarity = cos(세타) = A*B / |A||B|


[예시]  
A: 6월에 뉴턴은 선생님의 제안으로 트리니티에 입학했다.  
B: 6월에 뉴턴은 선생님의 제안으로 대학교에 입학했다.  
 
각 문장에 대하여 ['6월', '뉴턴', '선생님', '제안', '트리니티', '입학', '대학'] 의 토큰 값들은
A = [1, 1, 1, 1, 1, 1, 0]  
B = [1, 1, 1, 1, 0, 1, 1]  

[분자 계산]
A*B  = i=1 ~ n Ai * Bi의 합 = (1 * 1) + (1 * 1) + (1 * 1) + (1 * 1) + (1 * 0) + (1 * 1) + (0 * 1) = 1 + 1 + 1 + 1 + 0 + 1 + 0 = 5
 
[분모 계산]
|A||B| = (1^2 + 1^2 + 1^2 + 1^2 + 1^2 + 1^2 + 0^2)^1/2 * (1^2 + 1^2 + 1^2 + 1^2 + 0^2 + 1^2 + 1^2)^1/2 = 6^1/2 * 6^1/2 = 6

즉, 5/6이므로 두 문장은 `0.83333`만큼 유사도를 갖는다.  


# 6장. 챗봇 엔진에 필요한 딥러닝 모델

## 6.1 케라스(Keras)
여러 딥러닝 프레임워크가 있지만, `케라스(Keras)`는 직관적이고 사용하기 쉽다는 장점이 있다.  
무엇보다 `빠른 개발`을 목적으로 두고, 모듈 구성이 간단하여 비전문가들도 상대적으로 쉽게 사용할 수 있다.  
`케라스`는 `신경망 모델`을 구축할 수 있는 `고수준 API 라이브러리`이다.  
최근에 `텐서플로우 2.0`에 기본 API로 채택되어 구글의 지원을 받고있다.  

### 6.1.1 인공 신경망
`인공 신경망`은 두뇌의 `뉴런`을 모방한 모델이다.  

입력: x0, x1, x2  
가중치 : w0, w1, w2  

뉴런 : ∑wi*xi + b | f  
출력 : f(∑wi*xi + b)

뉴런의 계산 과정을 보면 입력된 xi값들과 대응되는 뉴런의 가중치 wi값들을 각각 곱해서 모두 더해준다.  
그리고 `편향값 b`를 통해 결과값을 조정한다.  
이를 수학적으로 표현하면 간단한 1차 함수의 모양이다.  
```
y = (w0x0 + w1x1 + w2x2) + b
```
실제 뉴런은 특정 강도 이상일 때만 다음 뉴런으로 신호를 전달하는데, 인공 신경망에서는 뉴런의 처리 과정 중 `f`로 표시된 영역이다.  
이를 `활성화 함수`라고 하며, 가중치 계산 결과값 y가 최종적으로 어떤 형태의 출력값으로 내보낼지 결정한다.  
활성화 함수는 종류가 많은데, 여기서는 3가지만 다룬다.  
1. 스텝 함수
`스텝 함수`는 가장 기본이 되는 함수이다.  
그래프가 `계단`같이 생겨서 스텝 함수이고, 입력값이 0보다 크면 1로, 0 이하일 때는 0으로 만든다.  
즉 입력값이 양수일 때만 활성화 시킨다.  

2. 시그모이드 함수
이진 분류에서 스텝 함수의 문제를 해결하기 위해 사용한다.  
스텝 함수에서 판단의 기준이 되는 `임계치` 부근의 데이터를 고려하지 않는 문제를 해결하기 위해 계단 모양을 완만한 형태로 표현했다.  
수식 : S(t) = 1 / (1 + e^-t)

3. ReLU 함수
입력값이 0 이상인 경우에는 기울기가 1인 직선이고, 0보다 작을 때는 결과값이 0이다.  
`시그모이드 함수`에 비해 연산 비용이 크지 않아 학습 속도가 빠르다.  

실제로 문제를 신경망 모델로 해결할 때는 1개의 뉴런만 사용하는 것이 아니라, 문제가 복잡할 수록 뉴런의 수가 늘어나고 신경망의 계층도 깊어진다.  

입력층과 출력층으로만 구성되어 있는 단순한 신경망을 `단층 신경망`이라고 한다.  
입력층과 1개 이상의 은닉층, 출력층으로 구성되어 있는 신경망은 `심층 신경망`이라고 한다.  
흔히 `딥러닝`과 `신경망`이라고 부르는 것들이 바로 `심층 신경망`이다.  
*신경망 계층이 `깊게(deep)`구성되어 각각의 뉴런을 `학습(learning)`시킨다 해서 딥러닝*

주로 `입력층`을 구성하는 뉴런들은 1개의 입력값을 갖고, 가중치와 활성화 함수를 갖고 있지 않아 입력된 값을 그대로 출력하는 특징이 있다.  
`출력층`의 뉴런은 각각 1개의 출력값을 갖고 있고, 지정된 활성화 함수에 따른 출력 범위를 갖고 있다.  
"복잡한 문제일 수록 뉴런와 `은닉층` 수를 늘리면 좋다"라고 하지만, 계산해야 하는 파라미터가 많아지면 비용이 높아지는 단점이 있다.  

신경망에 대해서 알아보자.  
신경망 모델에서 입력층으로부터 출력층까지 데이터가 순방향으로 전파되는 과정을 `순전파`라고 한다.  
데이터가 순방형으로 전파될 때 현 단계 뉴런의 가중치와 전 단계 뉴런의 출력값의 곱을 입력값으로 받는다.  
이 값은 활성화 함수를 통해 다음 뉴런으로~  

[순전파 예시]  
입력층: x --(전파: w_h * x)--> 은닉층: h --(전파: w_y * h)--> 결과값(yout)  

결과값이 실제 값과 오차가 많다면?  
다음 순전파 진행 시 오차가 줄어드는 방향으로 가중치(w_h, w_y)를 역방향으로 갱신해나간다.  
이 과정을 `역전파`라고 한다.  

### 6.1.2 딥러닝 분류 모델 만들기
[History 객체]  
loss : 각 에포크마다의 학습 손실값  
accuracy : 각 에포크마다의 학습 정확도  
val_loss : 각 에포크마다의 검증 손실값  
val_accuracy : 각 에포크마다의 검증 정확도  
epoch(에포크) : 학습의 횟수  

```
import tensorflow as tf
import matplotlib.pyplot as plt
from tensorflow.keras.datasets import mnist
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Flatten, Dense

# MNIST 데이터셋 가져오기
(x_train, y_train), (x_test, y_test) = mnist.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0 # 데이터 정규화

# tf.data를 사용하여 데이터셋을 섞고 배치 만들기
ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(10000)
train_size = int(len(x_train) * 0.7) # 학슴셋 : 검증셋 = 7 : 3
train_ds = ds.take(train_size).batch(20)
val_ds = ds.skip(train_size).batch(20)

# MNIST 분류 모델 구성
model = Sequential()
model.add(Flatten(input_shape=(28, 28)))
model.add(Dense(20, activation='relu'))
model.add(Dense(20, activation='relu'))
model.add(Dense(10, activation='softmax'))

# 모델 생성
model.compile(loss='sparse_categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])
# model.complie(loss='categorial_crossentropy', optimizer='sgd', metrics['accuracy'])

# 모델 학습
hist = model.fit(train_ds, validation_data=val_ds, epochs=10)

# 모델 평가
print('모델 평가')
model.evaluate(x_test, y_test)

# 모델 정보 출력
model.summary()

# 모델 저장
model.save('mnist_model.h5')

# 학습 결과 그래프 그리기
fig, loss_ax = plt.subplots()
acc_ax = loss_ax.twinx()

loss_ax.plot(hist.history['loss'], 'y', label='train loss')
loss_ax.plot(hist.history['var_loss'], 'r', label='var loss')

acc_ax.plot(hist.history['accuracy'], 'b', label='train acc')
acc_ax.plot(hist.history['val_accuracy'], 'g', label='val acc')

loss_ax.set_xlabel('epoch')
loss_ax.set_ylabel('loss')
acc_ax.set_ylabel('accuracy')

loss_ax.legend(loc='upper left')
acc_ax.legend(loc='lower left')
plt.show()
```  
위 코드를 돌리다가 만약 주피터에서 이런 오류가 뜬다면??
> The kernel appears to have died. It will restart automatically.

이 오류가 발생하는 이유는 `커널이 죽어서`인데, 커널이 죽는 이유는 `메모리 할당량을 초과`했기 때문!  
[해결방법]  
1. `jupyter_notebook_config.py`에서 다음 코드를 입력하고 재부팅하기.    
    - 위의 .py파일은 `./jupyter`폴더 안에 있음  
> c.NotebookApp.max_buffer_size = 100000000000000000000000

이렇게 큰 값을 입력해 바꾸면 됨



## 6.2.1 CNN 모델 개념
`CNN`을 이해하기 전에 `합성곱`과 `풀링`연산이 무엇인지 알아야 한다.  

[합성곱 연산]    
`합성곱 연산`이란, `합성곱 필터`로 불리는 특정 크기의 행렬을 이미지 데이터(or 문장 데이터) 행렬에 슬라이딩하면서 곱하고 더하는 연산을 의미함.  
`합성곱 필터`란 경우에 따라 마스크, 윈도우, 커널 등으로 다양하게 불린다.  
*이 책에서는 필터 혹은 커널로 부름*

[풀링 연산]  
`풀링 연산`이란, 합성곱 연산 결과로 나온 특징맵의 크기를 줄이거나 주요한 특징을 추출하기 위해 사용하는 연산으로, `최대 풀링`과 `평균 풀링`이 있다.  
이 중에서는 `최대 풀링 연산`을 주로 사용한다고 한다.  
`풀링 연산`에서도 합성곱 연산에서 사용하는 윈도우 크기, 스트라이드, 패딩 개념이 동일하게 적용된다.  *풀링 연산에서는 필터보단 윈도우라는 용어를 사용한다.*



## 6.3 개체명 인식을 위한 양방향  LSTM 모델
챗봇 엔진에 `개체명 인식`을 위해  사용하는 `양방향 LSTM`에 대해서 학습한다.  
`LSTM`은 순환 신경망 모델의 일종으로 시퀀스 또는 시계열 데이터의 패턴을 인식하는 분야에서 많이 사용한다.  
연속적인 데이터의 패턴을 이용해 결과를 예측하므로, 주로 주가 예측이나 신호 분석 및 번역 분야에서 좋은 성능을 보인다.  

### 6.3.1 RNN
`LSTM`은 `RNN`모델에서 파생된 모델이다.  
그래서 `RNN`이 뭔데?  
`RNN`은 순환 신경망으로 불리며, 앞서 배운 신경망 모델과 다르게 은닉층 노드의 출력값을 출력층과 그 다음 시점의 은닉층 노드의 입력으로 전달해 순환하는 특징을 갖고 있다.  

`RNN` 모델은 어떤 문제를 해결하느냐에 따라 입력과 출력의 길이를 조절할 수 있다.  
여러 개를 입력받아 하나를 출력하는 모델 : `many-to-one 모델`  
한 개를 입력받아 여러 개를 출력하는 모델 : `one-to-many 모델`  
여러 개를 입력받아 여러 개를 출력하는 모델 : `many-to-many 모델`   

참고로, `RNN`은 모든 시점에서 동일한 가중치와 편향값을 사용한다.  


### 6.3.2 LSTM
`RNN`은 입력 시퀀스의 시점이 길어질 수록 앞쪽의 데이터가 뒤쪽으로 잘 전달되지 않아 학습능력이 떨어진다는 단점이 있다.  
또한 RNN을 다층 구조로 쌓으면 입력과 출력 데이터 사이의 연관 관계가 줄어들어 장기 의존성 문제가 발생한다.  
이런 문제를 보완하기 위해 RNN을 변형시켜 나온 것이 바로 `LSTM`!!  

`LSTM`의 내부 구조를 살펴보자.  
`LSTM`은 기본적인 RNN의 은닉상태를 계산하는 방식에 변화가 있으며, 은닉 상탯값 이외에 셀 상태값이 추가되었다.  
은닉 상태값과 셀 상태값을 계산하기 위해 3개의 `게이트(Gate)`가 추가되었다.  
1. 입력 게이트
    - 현재 정보를 얼마나 기억할지 결정하는 게이트
2. 삭제 게이트
    - 이전 시점의 셀 상태값을 삭제하기 위해 사용되는 게이트
3. 출력 게이트
    - 출력 게이트의 결과값은 현재 은닉 시점의 은닉 상태를 결정하는데 사용되며, 해당 값은 전달되는 메모리 셀이 많아질수록 정보 유실이 크기 때문에 `단기 상태`라고 부름


### 6.3.3 양방향 LSTM
`RNN`이나 `LSTM`은 일반 신경망과 다르게 시퀀스 또는 시계열 데이터 처리에 특화되어 은닉층에서 과거의 정보를 기억할 수 있다.  
하지만 `순환 신경망`의 특성상 데이터가 입력 순으로 처리되기 때문에 이전 시점의 정보만 활용할 수 밖에 없다는 단점이 존재하는데...  

[예시]  
> ios앱 "개발"은 맥북이 필요합니다.  

일반적인 `RNN`이나 `LSTM`에서는 'ios'와 '앱'이라는 단어만으로 " "에 "개발"이라는 단어가 들어갈 것이라고 유추하기에는 정보가 부족하다.  
따라서 자연어 처리에 있어서 입력 데이터의 정방향 처리만큼 역방향 처리도 중요하다!! 이말입니다~  

`양방향 LSTM`은 기존 `LSTM계층`에 역방향으로 처리하는 `LSTM계층`을 하나 더 추가해 양방향에서 문장의 패턴을 분석할 수 있도록 되어있다.  
이러면 입력 문장을 양방향에서 처리하므로 문장이 길어진다고 하더라도 정보 손실 없이 처리가 가능하다고 한다!  

### 6.3.4 개체명 인식
`개체명 인식(NER)`이란 문장에서 각 개체의 `유형`을 인식하는 동작이다.  
즉, 문장 내에 포함된 어떤 단어가 인물, 장소, 날짜 등을 의미하는 단어인지 인식하는 것이다.  
딥러닝 모델이나 확률 모델 등을 이용해 문장에서 개체명을 인식하는 프로그램을 `개체명 인식기`라고 부른다!  

`개체명 인식`은 챗봇에서 문장을 정확하게 해석하기 위해 "반드시" 해야하는 `전처리 과정`이다!!  

개체명 인식기를 개발하는 것은 매우 어려운 과정이다...  
국내에는 공개되어있는 학습 데이터도 많지 않을 뿐더러, 개발하기 위해 학습 데이터를 만드는 것 또한 많은 시간과 비용이 들어가기 떄문..ㅜㅜ  

그래서 이 책에서는 완벽한 개체명 인식 모델은 아니더라도 순환 신경망을 이용헤 개체명을 인식하는 원리를 익힐 수 있도록 한다.  
약간의 문제가 존재하지만, 학습 데이터만 충분하다면 토이 챗봇을 만드는 데에는 무리가 없다고 한다!  

개체명 인시기 모델을 만들기 위해서는 먼저 `BIO 표기법`을 알아야 한다.  

`BIO 표기법`이란, "Beginning Inside, Outside"의 약자로, 각 토큰마다 태그를 붙이기 위해 사용한다.  
`B`는 개체명이 시작되는 단어에 `B-개체명`으로 태그되며, `I`는 `B-개체명`과 연결되는 단어일 때 `I-개체명`으로 태그된다.  
`O`는 개체명 이외의 모든 토큰에 태그된다.  

[예시]  
```
오늘부터 샤닐 길동은 삼성전자에 근무합니다.  

오늘 / B-Date  
부터 / O  
  
샤닐 / B-Person  
길동 / I-Person
은 / O  
  
삼성 / B-Company  
전자 / I-Company  
에 / O  
  
근무 / O  
합니다 / O
```  
위의 예시와 같이 두 개 이상의 토큰이 하나의 개체를 구성하는 경우가 많기 때문에 `BIO 표기법`을 사용하는 것이다.  


# 7장. 챗봇 학습 툴 만들기

## 7.1 MySQL
`MySQL`은 가장 많이 사용하는 오픈소스 `관계형 데이터베이스 관리 시스템 (RDBMS)`이다.  
파이썬을 포함한 다양한 언어에서 사용할 수 있도록 API를 지원하고 있다.  

## 7.2 파이썬으로 데이터베이스 연동하기
파이썬에서 mysql을 이용하려면 mysql 클라이언트 라이브러리를 사용해야 한다.  
하지만 `저수준 API`로 되어있어 직접적으로 사용하기엔 어려울 수도...  
하지만, `고수준 API`를 지원하고 있으며, 무료로 사용이 가능한 `PyMySQL`모듈이 있다!  

아래의 코드로 모듈 불러오기  
```
import pymysql
```  

### 7.2.1 데이터베이스 연결하기
데이터 조작을 위해서는 제일 먼저 MySQL 호스트 DB서버에 연결이 되어있어야 한다.  
```
db = pymysql.connect(
    host='127.0.0.1', 
    user='homestead', 
    passwd='secret', 
    db='homestead', 
    charset='utf8'
)
```  
host: 데이터베잇 서버가 존재하는 호스트 주소  
user: 데이터베이스 로그인 유저  
passwd: 데이터베이스 로그인 패스워드  
db: 데이터베이스 이름  
charset: 데이터베이스에서 사용할 charset 인코딩  

### 7.2.2 데이터 조작하기
데이터 조작 방법은 `DB테이블 생성`, 데이터 `삽입(INSERT)`, `조회(SELCET)`, `변경(UPDATE)`, `삭제(DELETE)` 등이 있다.  

파이썬 코드에 SQL문을 집어넣어 동작시키려면 아래와 같은 방식으로 해보자요.  
```commandline
sql = '''
    CREATE TABLE tb_student (
        id int primary key auto_increment not null,
        name varchar(32),
        age int,
        address varchar(32)
    ) ENGINE=InnoDB DEFAULT CHARSET=utf8
    '''
```  

## 7.3 챗봇 학습툴 만들기
학습 데이터를 DB에 저장했을 때 실시간으로 챗봇 시스템에 적용될 수 있도록 제작하는 것이 이번 목표!  
딥러닝 모델의 학습 데이터셋과는 개념이 조금 다르다.  
챗봇 엔진에는 `자연어 처리`를 위한 딥러닝 모델을 사용하는데 이 때 모델 학습을 위해 사용하는 데이터 처리는 8장에서 다룸..  

[챗봇 엔진 입력 처리 과정]  
```commandline
[문장] "내일 오전 10시에 탕수육 주문이 가능할까요?" ->  
[챗봇엔진] ->  
[엔진 해석 결과] 
1. 의도(Intent): 음식주문  
2. 개체명(Named Entity): 내일: Date, 오전 10시: Time, 탕수육: Food  
3. 키워드(Keyword): 내일, 오전, 10시, 탕수육, 주문
```  
두 번째 과정은 엔진에서 해석한 결과를 이용해 학습 DB 내용을 검색한다.  
이 때 해석 결과(의도, 개체명)에 매칭되는 답변 정보다 DB에 존재하면 데이터를 불러와 사용자에게 답변으로 제공.  
[챗봇 엔진 답변 처리 과정]  
```commandline
[챗봇엔진] 답변 검색(의도, 개체명) ->  
[학습 DB] 검색된 답변 데이터 ->  
[답변출력] "주문해주셔서 감사합니다."
```  

### 7.3.1 프로젝트 구조
chatbot
    - train_tools : 챗봇 학습 `툴` 관련 파일  
    - models : 챗봇 엔진에서 사용하는 `딥러닝 모델` 관련 파일
        - intent : `의도` 분류 모델 관련 파일
        - ner    : `개체` 인식 모델 관련 파일
    - utils : 챗봇 개발에 필요한 `유틸리티 라이브러리`
    - config : 챗봇 개발에 필요한 `설정`
    - test : 챗봇 개발에 필요한 `테스트 코드`  
  

`OpenPyXL`모듈을 사용하면 엑셀 파일을 읽어와 DB에 데이터를 저장할 수 있다.  
```commandline
    # 학습 엑셀 파일 불러오기
    wb = openpyxl.load_workbook(train_file)
    sheet = wb['Sheet1']
    for now in sheet.iter_rows(min_row=2):
        # 데이터 저장
        insert_data(db, xls_row=)

    wb.close()
```  

#### 7.3.2 당황스러운 오류
여기까지 프로젝트의 구조와 파일들을 작성하면서 오류가 발생했다...  
파이썬의 기초가 다져지지 않았다는 뜻인가..?  
오류를 모르겠다ㅜㅜ  

[파일 경로 오류]  
```commandline
from config.DatabaseConfig import *

```  
여기서 `config.Database`가 오류가 나는데, `config`에만 빨간 밑줄이 쳐진다.  
실행해보면, *No module named 'config'*의 오류 코드가 나옴..  
`pip install config`를 하라고 하지만 해도 안됨..

내 프로젝트 구조는  
```angular2html
chatbot
    - ch01 ~ ch12
    - chatbot
        - config
        - models
        - test
        - train_tools
        - utils
```

해결해버렸다!  
`pip install config`가 아니고! 경로 문제였다...  
내 프로젝트 구조는 `chatbot`이라는 최상위 폴더 아래에 `chatbot`폴더가 또 하나 있었고 이 폴더에 챗봇 프로젝트 코드가 들어있다.  
그러니까! import하려면 `from chatbot.config.DatabaseConfig import *`처럼 해야한다.. ㅎ.ㅎ 파이썬 기초 화이팅!


# 8장. 챗봇 엔진 만들기

## 8.1 챗봇 엔진이란
`챗봇`이란 *채팅 하는 봇*이라는 뜻이다.  
그러면 `챗봇 엔진`이란 뭘까?  
`챗봇 엔진`은 챗봇에서 핵심 기능을 하는 `모듈`이며, 화자의 질문을 이해하고 알맞은 
답변을 출력하는 역할을 한다.  

한 마디로 `자연어 처리 모듈`이라고 한다.  

## 8.2 챗봇 엔진 구조
챗봇을 설계하기 전에 내가 만들고자 하는 챗봇의 목적과 어떤 도메인 지식을 가지는 
챗봇을 만들 것인지 결정해야 한다.  
그에 따라 챗봇 엔진 개발 방법론과 학습에 필요한 데이터셋이 달라지기 때문에 매우 중요한 과정이다!!  

이 책에서 만드는 챗봇은 토이 수준의 챗봇이며, 이 챗봇 엔진은 토이 수준이기 때문에 
크게 5가지의 기능을 한다.  

[기능]  

| 핵심 기능     | 설명                                                                  |
|-----------|---------------------------------------------------------------------|
| 질문 의도 분류  | 화자의 질문 의도를 파악함. 의도 분류 모델을 이용해 의도 클래스를 예측하는 문제.                      |
| 개체명 인식    | 화자의 질문에서 단어 토큰별 개체명을 인식함. 단어 토큰에 맞는 개체명을 예측하는 문제.                   |
| 핵심 키워드 추출 | 화자의 질문 의미에서 핵심이 될 만한 단어 토큰을 추출. 형태소 분석기를 이용해 핵심 키워드가 되는 명사나 동사를 추출. |
| 답변 검색     | 해당 질문의 의도, 개체명, 핵심 키워드 등을 기반으로 답변을 학습 DB에서 검색.                      |
| 소켓 서버     | 다양한 종류의 챗봇 클라이언트에서 요청하는 질문을 처리하기 위해 소켓 서버 프로그램 역할. (챗봇 엔진 서버 프로그램)  |

[챗봇 엔진 처리 과정]  
```commandline
[질문]  
"내일 오전 10시에 탕수육 주문하고 싶어" -> 챗봇 엔진  

[챗봇 엔진]  
1. 전처리 과정 - 불용어 제거, 토큰 분리(키워드 추출) ->  
2. 의도 분석 - 의도: 음식 주문 -> 
3. 개체명 인식 - 내일: Data, 오전 10시: Time, 탕수육: Food -> 
4. 답변 검색 - (의도, 개체명, 키워드) -> 
5. 학습 DB - DB에서 적절한 답변 탐색 -> 
6. 답변 출력 -> 출력

[답변]  
"주문해주셔서 감사합니다."
```  
화자의 질의 문장이 입력되면 챗봇 엔진은 제일 먼저 `전처리`를 진행한다.  
형태소 분석기를 이용해 단어 토큰(키워드)을 추출한 뒤 명사나 동사 등 필요한 품사만 남기고 불용어는 제거한다.  
여기서, `불용어(Stopword)`란 언어 분[석시 의미가 있는 단어와 의미가 없는 단어나 조사 등이 있는데, 그 중에서 의미가 없는 것을 `불용어`라고 한다.  
그 다음 의도 분석과 개체명 인식을 완료한 후 결과값을 이용해 적절한 답변을 학습 DB에서 검색해 화자에게 답변을 출력함.  
  
해당 챗봇 엔진에는 자연어 처리를 위해 2가지 딥러닝 모델(의도 분석, 개체명 인식)을 사용한다.  
해당 `도메인 지식`에 맞는 딥러닝 모델 학습 데이터셋을 많이 보유하고 있다면 `성능`이 우수한 챗봇 엔진 개발에 도움이 된다!  
`성능`은 클라이언트의 질문에 대한 답변의 속도, 질문에 얼마나 적절한 답변을 출력하는가 등... 이 있는 것 같습니다.  
  
## 8.3 전처리 과정
챗봇에서 자연어를 처리할 때 가장 먼저 수행해야 하는 기본 과정인 `전처리 과정`은 어떻게 될까?  
전처리를 어떻게 하느냐에 따라 후처리 부분의 성능 차이가 커지므로 매우 중요하다고 한다!  

이 책에서 만드는 챗봇 엔진 전처리 과정은 `1) 형태소 분석기로 토크나이징`하고, `2) 문장 해석에 의미 있는 정보만 남기고`, `3) 나머지 불용어들은 제거`하는 
작업을 한다.  

전처리 과정을 담당하는 모듈은 유틸리티 라이브러리이기 때문에 */utils* 내에 생성한다.  


## 8.4 단어 사전 구축 및 시퀀스 생성
챗봇 엔진에서 의도 분류 및 개체명 인식 모델의 학습을 하려면 단어 사전을 구축해야 한다.  
`corpus.txt`와 `create_dict.py`는 */train_tools/dict/* 에 생성한다.  

## 8.5 의도 분류 모델
챗봇 엔진에 화자의 질의가 입력되었을 때 전처리 과정을 거친 후 해당 문장의 의도를 분류한다.  
이 때 문장을 의도 클래스 별로 분류하기 위해 `CNN모델`을 사용함!  

[챗봇 엔진의 의도 분류 클래스 종류]  

| 의도명 | 분류 클래스 | 설명                 |
|-----|--------|--------------------|
| 인사  | 0      | 텍스트가 인사말인 경우       |
| 욕설  | 1      | 텍스트가 욕설인 경우        |
| 주문  | 2      | 텍스트가 주문 관련 내용인 경우  |
| 예약  | 3      | 텍스트가 예약 관련 내용인 경우  |
| 기타  | 4      | 어떤 의도에도 포함되지 않는 경우 |  

*/config* 안에 GlobalParams.py 파일을 생성함.  

### 8.5.1 의도 분류 모델 학습
*/models/intent* 디렉토리에 `total_train_data.csv` 파일을 학습 데이터셋으로 사용함.  
같은 위치에 `train_model.py`파일을 생성함.  

### 8.5.2 의도 분류 모듈 생성
챗봇 엔진의 의도 분류 모듈을 만들어보자.  
이 모듈은 의도 분류 모델 파일을 활용해 입력되는 텍스트의 의도 클래스를 예측하는 기능을 
가지고 있다.  

해당 모듈은 `딥러닝 모델`이므로 *models/intent* 내에 `IntentModel.py`파일을 생성한다.  
다음으로 IntentModel 클래스를 테스트하는 코드를 작성해야 한다.  
IntentModel 객체를 생성해 새로운 유형의 문장을 분류한다.  
테스트 코드이므로, */test* 아래에 `model_intent_test.py`파일을 생성한다.  

## 8.6 개체명 인식 모델 학습
챗봇 엔진에 입력된 문장의 의도가 분류된 후  문장 내 `개체명 인식(NER)`을 진행한다.  
개체명 인식을 위해 `양방향 LSTM`모델을 사용한다.  

[개체명 종류]  

|개체명| 설명     |
|---|--------|
|B_FOOD| 음식     |
|B_DT, B_TI| 날짜, 시간 |
|B_PS| 사람     |
|B_OG| 조직, 회사 |
|B_LC| 지역     |
  

### 8.6.1 개체명 인식 모델 학습
*/models/ner* 디렉토리의 `ner_train.txt`파일을 학습데이터 셋으로 사용한다.  
같은 경로에 `train_model.py`파일을 생성한다.  
이 파일은 `ner_train.txt`파일을 읽어와 `NER(개체명 인식) 모델`을 생성하고 학습하는 코드임.  

### 8.6.2 개체명 인식 모듈 생성
챗봇 엔진의 개체명 인식 모듈을 만들어보자.  
이 모듈은 `개체명 인식 모델` 파일을 활용해 입력한 문장 내부의 개체명을 인식한다.  
이 모듈은 `딥러닝 모델`이므로 */models/ner* 아래에 `NerModel.py`파일을 생성한다.  

## 8.7 답변 검색
화자로부터 입력된 문장이 전처리, 의도 분류, 개체명 인식 과정을 거쳐 해석된 데이터를 기반으로 
적절한 답변을 학습 DB로부터 검색하는 방법을 다룬다.  
챗봇 엔진이 자연어 차리를 통해 해석한 문장을 토대로 유사한 답변을 검색하는 일은 매우 중요하다.  

### 8.7.1 데이터베이스 제어 모듈 생성
*/utils* 아래에 `Database.py`파일을 생성한다.  


### 8.7.2 답변 검색 모듈 생성
챗봇 엔진은 입력되는 문장을 전처리, 의도 분류, 개체명 인식 과정을 거쳐서 나온 자연어 해석 결과를 이용해 
학습 DB에서 적절한 답변을 검색한다.  
*/utils* 아래에 `FindAnswer.py` 파일을 생성하자.  

## 8.8 챗봇 엔진 서버 개발
이 책에서 목표로 하는 챗봇 엔진은 다양한 플랫폼에서 언제든 접속해서 사용할 수 있도록 서버용 프로그램으로 제작하는 것이다.  
핵심 기능은 앞서 구현한 부분을 그대로 사용하며, 이 번에는 서버 통신을 위한 기능을 구현할 것임!  
  
챗봇들이 챗봇 엔진 서버와 `TCP/IP`방식으로 통신할 것임!  

### 8.8.1 통신 프로토콜 정의
최근에는 서버와 클라이언트 간의 통신을 `JSON 문자열`형태로 많이 한다고 한다.  
`JSON`은 `KEY:VALUE`의 형식으로 이루어진 데이터 객체를 전달하기 위해 인간이 읽을 수 있는 텍스트를 사용하는 
개방형 표준 포맷이다.  

### 8.8.2 다중 접속을 위한 TCP 소켓 서버
지금까지는 `싱글 스레드`의 방식이었다..  
싱글 스레드는 다수의 챗봇 클라이언트 서비스 요청을 한 번에 처리할 수 없다.  
이미 다른 클라이언트가 서비스를 받고 있으면 챗봇 엔진의 응답을 받지 못하기 때문...  

이럴 때 필요한 것이 바로 `멀티 스레드`방식이다.  
이 방식으로 챗봇 엔진 서버를 만들게 되면 다중 클라이언트가 서비스를 받을 수 있다!  

*/utils* 아래에 `BotServer.py`파일을 생성하자.  
*루트 디렉토리* 에는 클라이언트 파일인 `bot.py`파일을 생성한다.  


### 8.8.3 챗봇 테스트 클라이언트 프로그램
*/test* 아라에 `chatbot_client_test.py`파일을 생성한다. 


# 9장. 챗봇 API 만들기

# 9.1 챗봇 API
8장에서 만든 챗봇 엔진 서버와 직접 통신해 카카오톡이나 네이버톡톡과 같은 다양한 메신저 플랫폼이 
챗봇 엔진의 기능을 사용할 수 있도록 `챗봇 API 서버`를 만든다.  

[챗봇 시스템 구조]  
카카오톡 혹은 네이버톡톡 -> 챗봇 API 서버 <--> 챗봇 엔진 서버 <--> 학습 DB 서버  
  

# 파이썬 Flask
`REST API`는 웹상에서 호출할 수 있도록 웹 애플리케이션 형태로 만들어야 한다.  
이 책에서는 파이썬-Flask를 사용하여 만든다.  
하나의 언어로 챗봇 엔진과 API서버를 구현하므로, 다른 언어를 배우는 과정에서 오는 비용을 줄일 수 있기 때문이다!  

### 9.2.1 Hello Flask

### 9.2.2 URI 동적 변수
REST API는 `HTTP 메서드(GET, POST, DELETE, PUT)`에 따라 `URI`를 호출한다.  
이 때 필요한 기능에 따라 URI에 동적으로 변수가 들어갈 수도 있다.  

### 9.2.3 기본적인 REST API 서비스 구현
REST API는 `HTTP 메서드(GET, POST, DELETE, PUT)`에 따라 `URI`를 호출한다.
이 절에서는 클라이언트로부터 요청이 들어왔을 때 HTTP 매서드별로 뷰 함수를 정의하는 방법을 알아보자!  
이 책에서는 주로 POST, GET만 다룬다고 한다ㅜㅜ  

[HTTP 매서드별 CRUD 동작 설명]  

| HTTP 메서드 | CRUD 동작 | 설명               |
|----------|---------|------------------|
| POST     | CREATE  | 서버 리소스를 생성할 때 사용 |
| GET      | READ    | 서버 리소스를 읽어올 때 사용 |
| PUT      | UPDATE  | 서버 리소스를 수정할 때 사용 |
| DELETE   | DELETE  | 서버 리소스를 삭제할 때 사용 |

"예제 9-3"을 실행해 Flask 서버를 구동하고 우리가 작성한 REST API가 잘 작동하는지 테스트가 필요하다.  
메서드가 'GET'인 경우에는 브라우저 상에서 해당 주소로 접속하면 작동 결과를 확인할 수 있지만, 
'POST'인 경우에는 따로 POST 전송 웹 애플리케이션을 만들어야 테스트할 수 있다.  
여기서는 `Talend API Test`튤울 사용하여 테스트할 것이다.  
크롬 브라우저에서 작동하는 무료 확장 프로그램임!   

## 9.3 챗봇 API 구현
나는 왜 Talend API Tester로 POST요청을 보내면 404에러가 뜰까...  
찾아봐야겠다!


