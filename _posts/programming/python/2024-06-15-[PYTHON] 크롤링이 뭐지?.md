---
layout: single
title: "📘 [Python] 크롤링이 뭐지?"
toc: true
toc_sticky: true
toc_label: "목차"
categories: python
excerpt: ""
tag: []
---

# Crawler의 뜻

- 기는 것
- 파충류

기어다니는게 왜 크콜링?

크롤링은 인터넷을 `기어다니면서` 데이터를 수집하는 과정이다. 그래서 크롤링!

## 웹 크롤러

`웹 크롤러` 는 웹 페이지의 데이터를 모아주는 소프트웨어임

그러면? 웹 크롤링은 크롤러를 사용해서 웹 페이지의 데이터를 추출해 내는 행위를 말한다!

# url에서 html 데이터 가져오기

```python
import requests

url = "http://www.daum.net"
response = requests.get(url)

print(response.text)
```

# BeautibulSoup 사용하기

```python
import requests
from bs4 import BeautifulSoup

url = "http://www.daum.net/"
response = requests.get(url)
# print(response.text)

print(BeautifulSoup(response.text, 'html.parser'))
```

- response.text랑 같은 결과가 나옴!
- 근데? response.text와 beautifulSoup로 가져온 데이터는 서로 다른 데이터임

## html에서 태그 데이터 가져오기

```python
import requests
from bs4 import BeautifulSoup

url = "http://www.daum.net/"
response = requests.get(url)
# print(response.text[:500])

soup = BeautifulSoup(response.text, 'html.parser')

print(soup.title)
print(soup.title.string)
```

### span 태그 파싱하기

```python
print(soup.span)
```

- 이렇게 하면 가장 상단의 span만 가져옴

### 모든 span 파싱하기

```python
print(soup.findAll('span'))
```

## 모든 <a> 태그 파싱하기

```python
from bs4 import BeautifulSoup
import requests

url = "http://www.daum.net/"
response = requests.get(url)
soup = BeautifulSoup(response.text, 'html.parser')

# file = open("daum.html","w")
# file.write(response.text)
# file.close()

# print(soup.title)
# print(soup.title.string)
# print(soup.span)
# print(soup.findAll('span'))

# html 문서에서 모든 a태그를 가져오는 코드
print(soup.findAll("a","link_favorsch"))
```

- 문서에서 모든 a태그 중에 link_favorsch를 가진 것만 가져와라